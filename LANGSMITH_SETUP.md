# LangSmith Tracing Setup Guide

## 🔍 What is LangSmith?

LangSmith is LangChain's observability platform that provides:
- **Real-time tracing** of LLM calls, token usage, and latency
- **Debugging tools** to inspect prompts, outputs, and errors
- **Performance analytics** across runs
- **Cost tracking** for API usage

---

## 🚀 Quick Setup (5 minutes)

### Step 1: Get Your API Key

1. Go to [smith.langchain.com](https://smith.langchain.com)
2. Sign up (free tier available)
3. Navigate to **Settings** → **API Keys**
4. Click **Create API Key**
5. Copy the key (you'll only see it once!)

### Step 2: Add to Environment Variables

Add these to your `.env` file:

```bash
# LangSmith Tracing (Optional - for debugging AI agent)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls_xxx_your_key_here
LANGCHAIN_PROJECT=CollabCanvas-AI-Agent
```

**Environment Variable Details**:
- `LANGCHAIN_TRACING_V2=true` - Enables tracing
- `LANGCHAIN_API_KEY` - Your LangSmith API key
- `LANGCHAIN_PROJECT` - Project name (organizes traces)

### Step 3: Restart Your Dev Server

```bash
# Kill current server
Ctrl+C

# Restart with new env vars
npm run dev
```

### Step 4: Verify It's Working

1. Open your app
2. Send a message to the AI assistant
3. Go to [smith.langchain.com](https://smith.langchain.com)
4. Click on your project ("CollabCanvas-AI-Agent")
5. You should see a new trace appear! 🎉

---

## 📊 Understanding Traces

### Trace Hierarchy

```
┌─────────────────────────────────────────┐
│ Run (Top-level execution)               │
│ ↓                                        │
│ ┌───────────────────────────────────┐   │
│ │ Chain (AgentExecutor)             │   │
│ │ ↓                                  │   │
│ │ ┌─────────────────────────────┐   │   │
│ │ │ LLM Call 1 (Planning)      │   │   │
│ │ │ - Input tokens: 4,505      │   │   │
│ │ │ - Output tokens: 20        │   │   │
│ │ │ - Latency: 2,180ms         │   │   │
│ │ └─────────────────────────────┘   │   │
│ │ ↓                                  │   │
│ │ ┌─────────────────────────────┐   │   │
│ │ │ Tool Call: generate_coords │   │   │
│ │ │ - Args: {count: 50}        │   │   │
│ │ │ - Output: [...coordinates] │   │   │
│ │ └─────────────────────────────┘   │   │
│ │ ↓                                  │   │
│ │ ┌─────────────────────────────┐   │   │
│ │ │ LLM Call 2 (Execution)     │   │   │
│ │ │ - Input tokens: 8,330      │   │   │
│ │ │ - Output tokens: 29        │   │   │
│ │ │ - Latency: 15,450ms        │   │   │
│ │ └─────────────────────────────┘   │   │
│ │ ↓                                  │   │
│ │ ┌─────────────────────────────┐   │   │
│ │ │ Tool Call: batch_create    │   │   │
│ │ │ - Args: {shapes: [...]}    │   │   │
│ │ │ - Output: Success          │   │   │
│ │ └─────────────────────────────┘   │   │
│ └───────────────────────────────────┘   │
└─────────────────────────────────────────┘
```

---

## 🎯 Key Metrics to Track

### 1. **Latency Breakdown**
- **Total run time**: End-to-end request duration
- **LLM latency**: Time spent waiting for OpenAI
- **Tool latency**: Time spent executing tools

**Where to find it**: Click on any trace → See timeline view

### 2. **Token Usage**
- **Prompt tokens**: Input sent to LLM
- **Completion tokens**: Output generated by LLM
- **Total tokens**: Sum of both (affects cost)

**Where to find it**: Click on LLM call → See "Token Usage" panel

### 3. **Cost Analysis**
- Per-request cost (based on token usage)
- Project-wide cost trends
- Cost by model (GPT-4o vs GPT-3.5)

**Where to find it**: Project dashboard → "Usage" tab

### 4. **Error Rates**
- Failed runs vs successful runs
- Error types and messages
- Stack traces for debugging

**Where to find it**: Project dashboard → Filter by "Failed"

---

## 🔧 Advanced Configuration

### Custom Tags for Filtering

Add tags to traces for easier filtering:

```javascript
// In aiAgent.js
import { traceable } from "langsmith/traceable";

export const executeAICommand = traceable(
  async (userMessage, chatHistory, canvasShapes) => {
    // Your existing code...
  },
  {
    name: "ai_assistant_command",
    metadata: {
      version: "1.0",
      environment: import.meta.env.MODE, // "development" or "production"
    },
    tags: ["canvas-operation", "ai-assistant"],
  }
);
```

### Trace Sampling (Reduce Costs)

Only trace a percentage of requests:

```javascript
// .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_SAMPLING_RATE=0.1  # Trace 10% of requests
```

### Disable Tracing in Production

```javascript
// Only trace in development
LANGCHAIN_TRACING_V2=${NODE_ENV === 'development' ? 'true' : 'false'}
```

---

## 📈 Analyzing Performance with LangSmith

### Scenario 1: Identify Slow Requests

**Goal**: Find which requests take >10 seconds

**Steps**:
1. Go to your project
2. Click "Runs" tab
3. Sort by "Latency" (descending)
4. Click on slow run → Expand timeline
5. Identify bottleneck (usually LLM call #2)

### Scenario 2: Compare Before/After Optimization

**Goal**: Measure impact of consolidated tool

**Steps**:
1. Add tags to differentiate approaches:
   ```javascript
   tags: ["approach:standard-agent"]  // Before
   tags: ["approach:consolidated-tool"]  // After
   ```
2. Go to project → "Analytics"
3. Filter by tag
4. Compare average latency

### Scenario 3: Debug Failed Requests

**Goal**: Understand why AI isn't executing correctly

**Steps**:
1. Go to project → Filter "Status: Failed"
2. Click on failed run
3. Expand error details
4. Check:
   - Input prompt (was it clear?)
   - Tool schema (did AI provide correct args?)
   - Error message (what went wrong?)

---

## 🎨 Custom Tracing for Your App

### Trace Individual Functions

```javascript
import { traceable } from "langsmith/traceable";

const generateExecutionPlan = traceable(
  async (userMessage, canvasShapes) => {
    // Your planning logic
    return plan;
  },
  {
    name: "generate_plan",
    run_type: "chain",
    tags: ["planning-phase"],
  }
);

const executePlan = traceable(
  async (plan, tools) => {
    // Your execution logic
    return results;
  },
  {
    name: "execute_plan",
    run_type: "tool",
    tags: ["execution-phase"],
  }
);
```

### Add Custom Metadata

```javascript
import { getCurrentRunTree } from "langsmith/run_trees";

// Inside your function
const runTree = getCurrentRunTree();
if (runTree) {
  runTree.metadata = {
    shapeCount: canvasShapes.length,
    planSteps: plan.length,
    userIntent: "create_random_shapes",
  };
}
```

---

## 📊 LangSmith Dashboard Widgets

### 1. Latency Trends
- **Widget**: Line chart
- **Y-axis**: Latency (ms)
- **X-axis**: Time
- **Filter**: Last 7 days

### 2. Token Usage Over Time
- **Widget**: Stacked bar chart
- **Y-axis**: Token count
- **Breakdown**: Prompt tokens vs Completion tokens
- **Filter**: This month

### 3. Error Rate
- **Widget**: Pie chart
- **Slices**: Success vs Failed
- **Filter**: Last 24 hours

### 4. Cost Per Request
- **Widget**: Histogram
- **X-axis**: Cost buckets ($0.01, $0.05, $0.10+)
- **Y-axis**: Request count

---

## 🔐 Security Best Practices

### 1. **Don't Commit API Keys**
```bash
# .gitignore should include:
.env
.env.local
```

### 2. **Use Different Projects for Dev/Prod**
```bash
# Development
LANGCHAIN_PROJECT=CollabCanvas-Dev

# Production
LANGCHAIN_PROJECT=CollabCanvas-Prod
```

### 3. **Rotate Keys Regularly**
- Rotate every 90 days
- Generate new key → Update `.env` → Delete old key

### 4. **Limit Team Access**
- Invite team members with appropriate roles
- Use "Viewer" role for read-only access

---

## 🆘 Troubleshooting

### Problem 1: No Traces Appearing

**Checklist**:
- ✅ `LANGCHAIN_TRACING_V2=true` in `.env`
- ✅ API key is correct (no typos)
- ✅ Dev server restarted after adding env vars
- ✅ At least one AI request sent after restart
- ✅ Check browser console for errors

**Debug**:
```javascript
// Add to aiAgent.js
console.log('LangSmith Tracing:', process.env.LANGCHAIN_TRACING_V2);
console.log('LangSmith API Key:', process.env.LANGCHAIN_API_KEY?.substring(0, 10) + '...');
```

### Problem 2: "Invalid API Key" Error

**Solution**:
1. Go to LangSmith → Settings → API Keys
2. Delete old key
3. Create new key
4. Update `.env` file
5. Restart server

### Problem 3: Traces Missing Tool Calls

**Reason**: Tools need to be wrapped with `traceable`

**Solution**:
```javascript
import { tool } from "@langchain/core/tools";

// Tools are automatically traceable when using @langchain/core/tools
const myTool = tool(async (input) => { ... }, { name: "my_tool" });
```

---

## 📚 Resources

- **Official Docs**: [docs.smith.langchain.com](https://docs.smith.langchain.com/)
- **Tracing Guide**: [Tracing](https://docs.smith.langchain.com/tracing)
- **Pricing**: [Free tier: 5K traces/month](https://smith.langchain.com/pricing)
- **Support**: [Discord](https://discord.gg/langchain)

---

## ✅ Verification Checklist

After setup, verify:
- [ ] `.env` has all three variables
- [ ] Dev server restarted
- [ ] Sent test message to AI assistant
- [ ] New trace visible in LangSmith dashboard
- [ ] Trace shows LLM calls with token usage
- [ ] Tool executions are visible in trace
- [ ] No errors in browser console

Once all checked, you're ready to use LangSmith for debugging and performance optimization! 🎉

