# LangSmith Tracing Setup Guide

## ðŸ” What is LangSmith?

LangSmith is LangChain's observability platform that provides:
- **Real-time tracing** of LLM calls, token usage, and latency
- **Debugging tools** to inspect prompts, outputs, and errors
- **Performance analytics** across runs
- **Cost tracking** for API usage

---

## ðŸš€ Quick Setup (5 minutes)

### Step 1: Get Your API Key

1. Go to [smith.langchain.com](https://smith.langchain.com)
2. Sign up (free tier available)
3. Navigate to **Settings** â†’ **API Keys**
4. Click **Create API Key**
5. Copy the key (you'll only see it once!)

### Step 2: Add to Environment Variables

Add these to your `.env` file:

```bash
# LangSmith Tracing (Optional - for debugging AI agent)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=ls_xxx_your_key_here
LANGCHAIN_PROJECT=CollabCanvas-AI-Agent
```

**Environment Variable Details**:
- `LANGCHAIN_TRACING_V2=true` - Enables tracing
- `LANGCHAIN_API_KEY` - Your LangSmith API key
- `LANGCHAIN_PROJECT` - Project name (organizes traces)

### Step 3: Restart Your Dev Server

```bash
# Kill current server
Ctrl+C

# Restart with new env vars
npm run dev
```

### Step 4: Verify It's Working

1. Open your app
2. Send a message to the AI assistant
3. Go to [smith.langchain.com](https://smith.langchain.com)
4. Click on your project ("CollabCanvas-AI-Agent")
5. You should see a new trace appear! ðŸŽ‰

---

## ðŸ“Š Understanding Traces

### Trace Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Run (Top-level execution)               â”‚
â”‚ â†“                                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Chain (AgentExecutor)             â”‚   â”‚
â”‚ â”‚ â†“                                  â”‚   â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚ â”‚ â”‚ LLM Call 1 (Planning)      â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Input tokens: 4,505      â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Output tokens: 20        â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Latency: 2,180ms         â”‚   â”‚   â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚ â”‚ â†“                                  â”‚   â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚ â”‚ â”‚ Tool Call: generate_coords â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Args: {count: 50}        â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Output: [...coordinates] â”‚   â”‚   â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚ â”‚ â†“                                  â”‚   â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚ â”‚ â”‚ LLM Call 2 (Execution)     â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Input tokens: 8,330      â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Output tokens: 29        â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Latency: 15,450ms        â”‚   â”‚   â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚ â”‚ â†“                                  â”‚   â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚ â”‚ â”‚ Tool Call: batch_create    â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Args: {shapes: [...]}    â”‚   â”‚   â”‚
â”‚ â”‚ â”‚ - Output: Success          â”‚   â”‚   â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Key Metrics to Track

### 1. **Latency Breakdown**
- **Total run time**: End-to-end request duration
- **LLM latency**: Time spent waiting for OpenAI
- **Tool latency**: Time spent executing tools

**Where to find it**: Click on any trace â†’ See timeline view

### 2. **Token Usage**
- **Prompt tokens**: Input sent to LLM
- **Completion tokens**: Output generated by LLM
- **Total tokens**: Sum of both (affects cost)

**Where to find it**: Click on LLM call â†’ See "Token Usage" panel

### 3. **Cost Analysis**
- Per-request cost (based on token usage)
- Project-wide cost trends
- Cost by model (GPT-4o vs GPT-3.5)

**Where to find it**: Project dashboard â†’ "Usage" tab

### 4. **Error Rates**
- Failed runs vs successful runs
- Error types and messages
- Stack traces for debugging

**Where to find it**: Project dashboard â†’ Filter by "Failed"

---

## ðŸ”§ Advanced Configuration

### Custom Tags for Filtering

Add tags to traces for easier filtering:

```javascript
// In aiAgent.js
import { traceable } from "langsmith/traceable";

export const executeAICommand = traceable(
  async (userMessage, chatHistory, canvasShapes) => {
    // Your existing code...
  },
  {
    name: "ai_assistant_command",
    metadata: {
      version: "1.0",
      environment: import.meta.env.MODE, // "development" or "production"
    },
    tags: ["canvas-operation", "ai-assistant"],
  }
);
```

### Trace Sampling (Reduce Costs)

Only trace a percentage of requests:

```javascript
// .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_SAMPLING_RATE=0.1  # Trace 10% of requests
```

### Disable Tracing in Production

```javascript
// Only trace in development
LANGCHAIN_TRACING_V2=${NODE_ENV === 'development' ? 'true' : 'false'}
```

---

## ðŸ“ˆ Analyzing Performance with LangSmith

### Scenario 1: Identify Slow Requests

**Goal**: Find which requests take >10 seconds

**Steps**:
1. Go to your project
2. Click "Runs" tab
3. Sort by "Latency" (descending)
4. Click on slow run â†’ Expand timeline
5. Identify bottleneck (usually LLM call #2)

### Scenario 2: Compare Before/After Optimization

**Goal**: Measure impact of consolidated tool

**Steps**:
1. Add tags to differentiate approaches:
   ```javascript
   tags: ["approach:standard-agent"]  // Before
   tags: ["approach:consolidated-tool"]  // After
   ```
2. Go to project â†’ "Analytics"
3. Filter by tag
4. Compare average latency

### Scenario 3: Debug Failed Requests

**Goal**: Understand why AI isn't executing correctly

**Steps**:
1. Go to project â†’ Filter "Status: Failed"
2. Click on failed run
3. Expand error details
4. Check:
   - Input prompt (was it clear?)
   - Tool schema (did AI provide correct args?)
   - Error message (what went wrong?)

---

## ðŸŽ¨ Custom Tracing for Your App

### Trace Individual Functions

```javascript
import { traceable } from "langsmith/traceable";

const generateExecutionPlan = traceable(
  async (userMessage, canvasShapes) => {
    // Your planning logic
    return plan;
  },
  {
    name: "generate_plan",
    run_type: "chain",
    tags: ["planning-phase"],
  }
);

const executePlan = traceable(
  async (plan, tools) => {
    // Your execution logic
    return results;
  },
  {
    name: "execute_plan",
    run_type: "tool",
    tags: ["execution-phase"],
  }
);
```

### Add Custom Metadata

```javascript
import { getCurrentRunTree } from "langsmith/run_trees";

// Inside your function
const runTree = getCurrentRunTree();
if (runTree) {
  runTree.metadata = {
    shapeCount: canvasShapes.length,
    planSteps: plan.length,
    userIntent: "create_random_shapes",
  };
}
```

---

## ðŸ“Š LangSmith Dashboard Widgets

### 1. Latency Trends
- **Widget**: Line chart
- **Y-axis**: Latency (ms)
- **X-axis**: Time
- **Filter**: Last 7 days

### 2. Token Usage Over Time
- **Widget**: Stacked bar chart
- **Y-axis**: Token count
- **Breakdown**: Prompt tokens vs Completion tokens
- **Filter**: This month

### 3. Error Rate
- **Widget**: Pie chart
- **Slices**: Success vs Failed
- **Filter**: Last 24 hours

### 4. Cost Per Request
- **Widget**: Histogram
- **X-axis**: Cost buckets ($0.01, $0.05, $0.10+)
- **Y-axis**: Request count

---

## ðŸ” Security Best Practices

### 1. **Don't Commit API Keys**
```bash
# .gitignore should include:
.env
.env.local
```

### 2. **Use Different Projects for Dev/Prod**
```bash
# Development
LANGCHAIN_PROJECT=CollabCanvas-Dev

# Production
LANGCHAIN_PROJECT=CollabCanvas-Prod
```

### 3. **Rotate Keys Regularly**
- Rotate every 90 days
- Generate new key â†’ Update `.env` â†’ Delete old key

### 4. **Limit Team Access**
- Invite team members with appropriate roles
- Use "Viewer" role for read-only access

---

## ðŸ†˜ Troubleshooting

### Problem 1: No Traces Appearing

**Checklist**:
- âœ… `LANGCHAIN_TRACING_V2=true` in `.env`
- âœ… API key is correct (no typos)
- âœ… Dev server restarted after adding env vars
- âœ… At least one AI request sent after restart
- âœ… Check browser console for errors

**Debug**:
```javascript
// Add to aiAgent.js
console.log('LangSmith Tracing:', process.env.LANGCHAIN_TRACING_V2);
console.log('LangSmith API Key:', process.env.LANGCHAIN_API_KEY?.substring(0, 10) + '...');
```

### Problem 2: "Invalid API Key" Error

**Solution**:
1. Go to LangSmith â†’ Settings â†’ API Keys
2. Delete old key
3. Create new key
4. Update `.env` file
5. Restart server

### Problem 3: Traces Missing Tool Calls

**Reason**: Tools need to be wrapped with `traceable`

**Solution**:
```javascript
import { tool } from "@langchain/core/tools";

// Tools are automatically traceable when using @langchain/core/tools
const myTool = tool(async (input) => { ... }, { name: "my_tool" });
```

---

## ðŸ“š Resources

- **Official Docs**: [docs.smith.langchain.com](https://docs.smith.langchain.com/)
- **Tracing Guide**: [Tracing](https://docs.smith.langchain.com/tracing)
- **Pricing**: [Free tier: 5K traces/month](https://smith.langchain.com/pricing)
- **Support**: [Discord](https://discord.gg/langchain)

---

## âœ… Verification Checklist

After setup, verify:
- [ ] `.env` has all three variables
- [ ] Dev server restarted
- [ ] Sent test message to AI assistant
- [ ] New trace visible in LangSmith dashboard
- [ ] Trace shows LLM calls with token usage
- [ ] Tool executions are visible in trace
- [ ] No errors in browser console

Once all checked, you're ready to use LangSmith for debugging and performance optimization! ðŸŽ‰

